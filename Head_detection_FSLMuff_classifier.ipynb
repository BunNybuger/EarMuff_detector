{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Earmuff detector with Few Shot Learning\n",
    "This repo is an earmuff detector. It can get video stream from CCTV in workplaces and detect if the person in the video is using an earmuff or not.\n",
    "First, a trained YoloV5 model detects the person in the video; then, another YoloV5 model detects the person's head. The detected box of a head is sent to a PrototypicalNetwork, which was not trained to detect earmuff previously. The accuracy is acceptable, as you can see in the sample video. However, it should be noted that I have used some pictures of this video as the support set, and this will increase the accuracy but may not be possible in most practical situations.\n",
    "I take the Few Shot Learning(FSL) method because we don't have a dataset. I used efficientnet_b2 network architecture pre-trained on the ImageNet dataset as a feature extractor. In a later work, after a proper dataset is ready for this purpose, another method will be tested in reported here.\n",
    "\n",
    "[Here is the FSL repository](https://github.com/sicara/easy-few-shot-learning) from GitHub that I used. Thanks to @ebennequin, there is a good explanation of few-shot learning in the repository as well. you can [access the google colab file here](https://colab.research.google.com/github/sicara/easy-few-shot-learning/blob/master/notebooks/my_first_few_shot_classifier.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# import tensorflow as tf \n",
    "import numpy as np\n",
    "from models.experimental import attempt_load\n",
    "from utils.datasets import LoadStreams, LoadImages\n",
    "from utils.general import non_max_suppression \n",
    "import torch\n",
    "from utils.general import check_img_size, check_requirements, check_imshow, colorstr, non_max_suppression, \\\n",
    "    apply_classifier, scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few Shot Learning Model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional\n",
    "convert_tensor = transforms.ToTensor()\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import Omniglot\n",
    "from torchvision.models import efficientnet_b2 #resnet18\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from EasyFSL.easyfsl.data_tools import EasySet, TaskSampler\n",
    "from EasyFSL.easyfsl.utils import plot_images, sliding_average\n",
    "\n",
    "name_classes = ['Muff', 'NotMuff']\n",
    "color_list = [(0,255,0),(0,0,255)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_10516/2705736762.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\noor\\AppData\\Local\\Temp/ipykernel_10516/2705736762.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    pip3 freeze requirements.txt\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip3 freeze requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at c:\\Users\\noor\\.conda\\envs\\last_tf:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "absl-py                   0.14.1                   pypi_0    pypi\n",
      "argon2-cffi               20.1.0           py38h2bbff1b_1  \n",
      "astunparse                1.6.3                    pypi_0    pypi\n",
      "async_generator           1.10               pyhd3eb1b0_0  \n",
      "attrs                     21.2.0             pyhd3eb1b0_0  \n",
      "backcall                  0.2.0              pyhd3eb1b0_0  \n",
      "blas                      1.0                         mkl  \n",
      "bleach                    4.0.0              pyhd3eb1b0_0  \n",
      "blosc                     1.21.0               h19a0ad4_0  \n",
      "bottleneck                1.3.2            py38h2a96729_1  \n",
      "brotli                    1.0.9                ha925a31_2  \n",
      "bzip2                     1.0.8                he774522_0  \n",
      "ca-certificates           2022.5.18.1          h5b45459_0    conda-forge\n",
      "cachetools                4.2.4                    pypi_0    pypi\n",
      "certifi                   2022.5.18.1      py38haa244fe_0    conda-forge\n",
      "cffi                      1.14.6           py38h2bbff1b_0  \n",
      "cfitsio                   3.470                he774522_6  \n",
      "charls                    2.2.0                h6c2663c_0  \n",
      "charset-normalizer        2.0.7                    pypi_0    pypi\n",
      "clang                     5.0                      pypi_0    pypi\n",
      "cloudpickle               2.0.0              pyhd3eb1b0_0  \n",
      "colorama                  0.4.4              pyhd3eb1b0_0  \n",
      "console_shortcut          0.1.1                         4  \n",
      "cudatoolkit               10.2.89              h74a9793_1  \n",
      "cutmix-keras              1.0.0                    pypi_0    pypi\n",
      "cycler                    0.10.0                   py38_0  \n",
      "cytoolz                   0.11.0           py38he774522_0  \n",
      "dask-core                 2021.10.0          pyhd3eb1b0_0  \n",
      "debugpy                   1.4.1            py38hd77b12b_0  \n",
      "decorator                 5.1.0              pyhd3eb1b0_0  \n",
      "defusedxml                0.7.1              pyhd3eb1b0_0  \n",
      "docopt                    0.6.2                    pypi_0    pypi\n",
      "easyfsl                   0.2.0                    pypi_0    pypi\n",
      "entrypoints               0.3                      py38_0  \n",
      "flatbuffers               1.12                     pypi_0    pypi\n",
      "fonttools                 4.25.0             pyhd3eb1b0_0  \n",
      "freetype                  2.10.4               hd328e21_0  \n",
      "fsspec                    2021.10.1          pyhd3eb1b0_0  \n",
      "gast                      0.4.0                    pypi_0    pypi\n",
      "giflib                    5.2.1                h62dcd97_0  \n",
      "google-auth               1.35.0                   pypi_0    pypi\n",
      "google-auth-oauthlib      0.4.6                    pypi_0    pypi\n",
      "google-pasta              0.2.0                    pypi_0    pypi\n",
      "grpcio                    1.41.0                   pypi_0    pypi\n",
      "h5py                      3.1.0                    pypi_0    pypi\n",
      "hdf5                      1.10.4               h7ebc959_0  \n",
      "icc_rt                    2019.0.0             h0cc432a_1  \n",
      "icu                       58.2                 ha925a31_3  \n",
      "idna                      3.2                      pypi_0    pypi\n",
      "imagecodecs               2021.8.26        py38ha1f97ea_0  \n",
      "imageio                   2.9.0              pyhd3eb1b0_0  \n",
      "importlib-metadata        4.8.1            py38haa95532_0  \n",
      "importlib_metadata        4.8.1                hd3eb1b0_0  \n",
      "intel-openmp              2021.4.0          haa95532_3556  \n",
      "ipykernel                 6.4.1            py38haa95532_1  \n",
      "ipython                   7.29.0           py38hd4e2768_0  \n",
      "ipython_genutils          0.2.0              pyhd3eb1b0_1  \n",
      "jedi                      0.18.0           py38haa95532_1  \n",
      "jinja2                    3.0.2              pyhd3eb1b0_0  \n",
      "joblib                    1.1.0              pyhd3eb1b0_0  \n",
      "jpeg                      9d                   h2bbff1b_0  \n",
      "jsonschema                3.2.0              pyhd3eb1b0_2  \n",
      "jupyter_client            7.0.1              pyhd3eb1b0_0  \n",
      "jupyter_core              4.8.1            py38haa95532_0  \n",
      "jupyterlab_pygments       0.1.2                      py_0  \n",
      "jupyterthemes             0.20.0                     py_1    conda-forge\n",
      "keras                     2.6.0                    pypi_0    pypi\n",
      "keras-preprocessing       1.1.2                    pypi_0    pypi\n",
      "kiwisolver                1.3.1            py38hd77b12b_0  \n",
      "lcms2                     2.12                 h83e58a3_0  \n",
      "lerc                      3.0                  hd77b12b_0  \n",
      "lesscpy                   0.15.0             pyhd8ed1ab_0    conda-forge\n",
      "libaec                    1.0.4                h33f27b4_1  \n",
      "libdeflate                1.8                  h2bbff1b_5  \n",
      "libopencv                 4.0.1                hbb9e17c_0  \n",
      "libpng                    1.6.37               h2a8f88b_0  \n",
      "libtiff                   4.2.0                hd0e1b90_0  \n",
      "libuv                     1.40.0               he774522_0  \n",
      "libwebp                   1.2.0                h2bbff1b_0  \n",
      "libzopfli                 1.0.3                ha925a31_0  \n",
      "locket                    0.2.1            py38haa95532_1  \n",
      "loguru                    0.5.3                    pypi_0    pypi\n",
      "lz4-c                     1.9.3                h2bbff1b_1  \n",
      "m2w64-gcc-libgfortran     5.3.0                         6  \n",
      "m2w64-gcc-libs            5.3.0                         7  \n",
      "m2w64-gcc-libs-core       5.3.0                         7  \n",
      "m2w64-gmp                 6.1.0                         2  \n",
      "m2w64-libwinpthread-git   5.0.0.4634.697f757               2  \n",
      "markdown                  3.3.4                    pypi_0    pypi\n",
      "markupsafe                2.0.1            py38h2bbff1b_0  \n",
      "matplotlib                3.4.3            py38haa95532_0  \n",
      "matplotlib-base           3.4.3            py38h49ac443_0  \n",
      "matplotlib-inline         0.1.2              pyhd3eb1b0_2  \n",
      "mistune                   0.8.4           py38he774522_1000  \n",
      "mkl                       2021.4.0           haa95532_640  \n",
      "mkl-service               2.4.0            py38h2bbff1b_0  \n",
      "mkl_fft                   1.3.1            py38h277e83a_0  \n",
      "mkl_random                1.2.2            py38hf11a4ad_0  \n",
      "msys2-conda-epoch         20160418                      1  \n",
      "munkres                   1.1.4                      py_0  \n",
      "nbclient                  0.5.3              pyhd3eb1b0_0  \n",
      "nbconvert                 6.1.0            py38haa95532_0  \n",
      "nbformat                  5.1.3              pyhd3eb1b0_0  \n",
      "nest-asyncio              1.5.1              pyhd3eb1b0_0  \n",
      "networkx                  2.6.3              pyhd3eb1b0_0  \n",
      "notebook                  6.4.5            py38haa95532_0  \n",
      "numexpr                   2.7.3            py38hb80d3ca_1  \n",
      "numpy                     1.19.5                   pypi_0    pypi\n",
      "numpy-base                1.21.2           py38h0829f74_0  \n",
      "oauthlib                  3.1.1                    pypi_0    pypi\n",
      "olefile                   0.46               pyhd3eb1b0_0  \n",
      "opencv                    4.0.1            py38h2a7c758_0  \n",
      "opencv-python             4.5.4.58                 pypi_0    pypi\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.openjpeg                  2.4.0                h4fc8c34_0  \n",
      "\n",
      "openssl                   1.1.1o               h8ffe710_0    conda-forge\n",
      "opt-einsum                3.3.0                    pypi_0    pypi\n",
      "packaging                 21.0               pyhd3eb1b0_0  \n",
      "pandas                    1.3.3            py38h6214cd6_0  \n",
      "pandocfilters             1.4.3            py38haa95532_1  \n",
      "parso                     0.8.2              pyhd3eb1b0_0  \n",
      "partd                     1.2.0              pyhd3eb1b0_0  \n",
      "pickleshare               0.7.5           pyhd3eb1b0_1003  \n",
      "pillow                    8.4.0            py38hd45dc43_0  \n",
      "pip                       21.2.2           py38haa95532_0  \n",
      "pipreqs                   0.4.11                   pypi_0    pypi\n",
      "ply                       3.11                       py_1    conda-forge\n",
      "prometheus_client         0.11.0             pyhd3eb1b0_0  \n",
      "prompt-toolkit            3.0.20             pyhd3eb1b0_0  \n",
      "protobuf                  3.18.1                   pypi_0    pypi\n",
      "py-opencv                 4.0.1            py38he44ac1e_0  \n",
      "pyasn1                    0.4.8                    pypi_0    pypi\n",
      "pyasn1-modules            0.2.8                    pypi_0    pypi\n",
      "pycparser                 2.20                       py_2  \n",
      "pygments                  2.10.0             pyhd3eb1b0_0  \n",
      "pyparsing                 3.0.4              pyhd3eb1b0_0  \n",
      "pyqt                      5.9.2            py38ha925a31_4  \n",
      "pyrsistent                0.17.3           py38he774522_0  \n",
      "python                    3.8.5                h5fd99cc_1  \n",
      "python-dateutil           2.8.2              pyhd3eb1b0_0  \n",
      "python_abi                3.8                      2_cp38    conda-forge\n",
      "pytorch                   1.10.0          py3.8_cuda10.2_cudnn7_0    pytorch\n",
      "pytorch-mutex             1.0                        cuda    pytorch\n",
      "pytz                      2021.3             pyhd3eb1b0_0  \n",
      "pywavelets                1.1.1            py38he774522_2  \n",
      "pywin32                   228              py38hbaba5e8_1  \n",
      "pywinpty                  0.5.7                    py38_0  \n",
      "pyyaml                    5.4.1            py38h2bbff1b_1  \n",
      "pyzmq                     22.2.1           py38hd77b12b_1  \n",
      "qt                        5.9.7            vc14h73c81de_0  \n",
      "requests                  2.26.0                   pypi_0    pypi\n",
      "requests-oauthlib         1.3.0                    pypi_0    pypi\n",
      "rsa                       4.7.2                    pypi_0    pypi\n",
      "scikit-image              0.18.3           py38hf11a4ad_0  \n",
      "scikit-learn              0.24.2           py38hf11a4ad_1  \n",
      "scipy                     1.7.1            py38hbe87c03_2  \n",
      "seaborn                   0.11.2             pyhd3eb1b0_0  \n",
      "send2trash                1.8.0              pyhd3eb1b0_1  \n",
      "setuptools                58.0.4           py38haa95532_0  \n",
      "sip                       4.19.13          py38ha925a31_0  \n",
      "six                       1.15.0                   pypi_0    pypi\n",
      "snappy                    1.1.8                h33f27b4_0  \n",
      "sqlite                    3.36.0               h2bbff1b_0  \n",
      "tensorboard               2.6.0                    pypi_0    pypi\n",
      "tensorboard-data-server   0.6.1                    pypi_0    pypi\n",
      "tensorboard-plugin-wit    1.8.0                    pypi_0    pypi\n",
      "tensorflow                2.6.0                    pypi_0    pypi\n",
      "tensorflow-estimator      2.6.0                    pypi_0    pypi\n",
      "tensorflow-gpu            2.6.1                    pypi_0    pypi\n",
      "termcolor                 1.1.0                    pypi_0    pypi\n",
      "terminado                 0.9.4            py38haa95532_0  \n",
      "testpath                  0.5.0              pyhd3eb1b0_0  \n",
      "threadpoolctl             2.2.0              pyh0d69192_0  \n",
      "tifffile                  2021.7.2           pyhd3eb1b0_2  \n",
      "tk                        8.6.11               h2bbff1b_0  \n",
      "toolz                     0.11.2             pyhd3eb1b0_0  \n",
      "torchaudio                0.10.0               py38_cu102    pytorch\n",
      "torchvision               0.11.1               py38_cu102    pytorch\n",
      "tornado                   6.1              py38h2bbff1b_0  \n",
      "tqdm                      4.62.2             pyhd3eb1b0_1  \n",
      "traitlets                 5.1.0              pyhd3eb1b0_0  \n",
      "typing-extensions         3.7.4.3                  pypi_0    pypi\n",
      "typing_extensions         3.10.0.2           pyh06a4308_0  \n",
      "urllib3                   1.26.7                   pypi_0    pypi\n",
      "vc                        14.2                 h21ff451_1  \n",
      "vs2015_runtime            14.27.29016          h5e58377_2  \n",
      "wcwidth                   0.2.5              pyhd3eb1b0_0  \n",
      "webencodings              0.5.1                    py38_1  \n",
      "werkzeug                  2.0.2                    pypi_0    pypi\n",
      "wheel                     0.37.0             pyhd3eb1b0_1  \n",
      "win32-setctime            1.0.4                    pypi_0    pypi\n",
      "wincertstore              0.2              py38haa95532_2  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "winpty                    0.4.3                         4  \n",
      "wrapt                     1.12.1                   pypi_0    pypi\n",
      "xz                        5.2.5                h62dcd97_0  \n",
      "yaml                      0.2.5                he774522_0  \n",
      "yarg                      0.1.9                    pypi_0    pypi\n",
      "zfp                       0.5.5                hd77b12b_6  \n",
      "zipp                      3.6.0              pyhd3eb1b0_0  \n",
      "zlib                      1.2.11               h62dcd97_4  \n",
      "zstd                      1.4.9                h19a0ad4_0  \n"
     ]
    }
   ],
   "source": [
    "conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nvcc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10516/177488625.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnvcc\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'nvcc' is not defined"
     ]
    }
   ],
   "source": [
    "nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.0'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at c:\\Users\\noor\\.conda\\envs\\last_tf:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "cudatoolkit               10.2.89              h74a9793_1  \n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda list cudatoolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrototypicalNetworks(nn.Module):\n",
    "    def __init__(self, backbone: nn.Module):\n",
    "        super(PrototypicalNetworks, self).__init__()\n",
    "        self.backbone = backbone\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        support_images: torch.Tensor,\n",
    "        support_labels: torch.Tensor,\n",
    "        query_images: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predict query labels using labeled support images.\n",
    "        \"\"\"\n",
    "        # Extract the features of support and query images\n",
    "        z_support = self.backbone.forward(support_images)\n",
    "        z_query = self.backbone.forward(query_images)\n",
    "\n",
    "        # Infer the number of different classes from the labels of the support set\n",
    "        n_way = len(torch.unique(support_labels))\n",
    "        # Prototype i is the mean of all instances of features corresponding to labels == i\n",
    "        z_proto = torch.cat(\n",
    "            [\n",
    "                z_support[torch.nonzero(support_labels == label)].mean(0)\n",
    "                for label in range(n_way)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Compute the euclidean distance from queries to prototypes\n",
    "        dists = torch.cdist(z_query, z_proto)\n",
    "\n",
    "        # And here is the super complicated operation to transform those distances into classification scores!\n",
    "        scores = -dists\n",
    "        return scores\n",
    "\n",
    "\n",
    "convolutional_network = efficientnet_b2(pretrained=True)\n",
    "convolutional_network.fc = nn.Flatten()\n",
    "# print(convolutional_network)\n",
    "\n",
    "model_muff = PrototypicalNetworks(convolutional_network).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Constant Support Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_N_WAY = 2 # Number of classes in a task\n",
    "C_N_SHOT = 2 # Number of images per class in the support set\n",
    "C_N_QUERY = 0 # Number of images per class in the query set\n",
    "C_N_EVALUATION_TASKS = 1\n",
    "\n",
    "constant_support_set = EasySet(specs_file=\"./Support Data/ConstantSupportDataset.json\", training=False)\n",
    "test_sampler = TaskSampler(\n",
    "    constant_support_set, n_way=C_N_WAY, n_shot=C_N_SHOT, n_query=C_N_QUERY, n_tasks=C_N_EVALUATION_TASKS\n",
    ")\n",
    "\n",
    "constant_support_data_loader = DataLoader(\n",
    "    constant_support_set,\n",
    "    batch_sampler=test_sampler,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    collate_fn=test_sampler.episodic_collate_fn,\n",
    ")\n",
    "\n",
    "(   constant_support_images,\n",
    "    constant_support_labels,\n",
    "    example1_query_images,\n",
    "    example1_query_labels,\n",
    "    example1_class_ids,\n",
    ") = next(iter(constant_support_data_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load video YoloV5 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "Model Summary: 396 layers, 35462484 parameters, 0 gradients\n",
      "Fusing layers... \n",
      "Model Summary: 290 layers, 20852934 parameters, 0 gradients\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def resize_with_pad(img, new_w, new_h):\n",
    "\n",
    "    h,w,_ = img.shape\n",
    "\n",
    "    if h > w:\n",
    "        pad_size = h-w\n",
    "        top, bottom = 0,0\n",
    "        left, right = pad_size//2, pad_size//2\n",
    "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, None, value = 0)\n",
    "\n",
    "    elif w > h:\n",
    "        pad_size = w-h\n",
    "        top, bottom = pad_size//2, pad_size//2\n",
    "        left, right = 0,0\n",
    "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, None, value = 0)\n",
    "\n",
    "    img = cv2.resize(img, (new_w,new_h))\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "model_PersonDetector = attempt_load('./Person_Detector.pt', map_location='cuda')  # load FP32 model\n",
    "model_HeadDetector = attempt_load('./Head_detector_fromPersonBox_yolov5.pt', map_location='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================= Opens the Video file =======================\n",
    "video_folder='./Test Video/'\n",
    "video_name = 'GunShop_Trim'\n",
    "dataset = LoadImages(video_folder + video_name +'.mp4', img_size=640, stride=64)\n",
    "\n",
    "expand_headbox_width = 1.2 # multiplied to head box width to include earmuff\n",
    "\n",
    "cap = cv2.VideoCapture(video_folder + video_name +\".mp4\")\n",
    "if (cap.isOpened()== False):\n",
    "    print(\"Error opening video file\")\n",
    "video_fps = int(cap.get(5))\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "\n",
    "# for path, img, im0s, vid_cap, s in dataset:\n",
    "\n",
    "#     video_fps = int(vid_cap.get(5))\n",
    "#     frame_width = int(vid_cap.get(3))\n",
    "#     frame_height = int(vid_cap.get(4))\n",
    "#     break\n",
    "\n",
    "frame_save_path = video_folder+video_name\n",
    "head_save_path = video_folder+video_name +\"/head\"\n",
    "Muff_save_path = video_folder + video_name+\"/Muff\"\n",
    "NotMuff_save_path = video_folder + video_name+\"/NotMuff\"\n",
    "\n",
    "if not os.path.exists(frame_save_path):\n",
    "    os.mkdir(frame_save_path)\n",
    "\n",
    "vid_writer = cv2.VideoWriter(frame_save_path +\"/\" + video_name + '_Out.mp4',cv2.VideoWriter_fourcc(*'mp4v'), video_fps, (frame_width,frame_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================= For each frame does prepration and detects =======================\n",
    "\n",
    "if not os.path.exists(head_save_path):\n",
    "    os.mkdir(head_save_path)\n",
    "if not os.path.exists(Muff_save_path):\n",
    "    os.mkdir(Muff_save_path)\n",
    "if not os.path.exists(NotMuff_save_path):\n",
    "    os.mkdir(NotMuff_save_path)\n",
    "\n",
    "counter = 0\n",
    "for path, img, im0s, vid_cap, s in dataset:\n",
    "    counter += 1\n",
    "    if counter%1==0:\n",
    "        img = torch.from_numpy(img).to('cuda')\n",
    "        im0s_tensortorch = img\n",
    "        img = img.float()  # uint8 to fp16/32\n",
    "        img /= 255.0  \n",
    "        if len(img.shape) == 3:\n",
    "            img = img[None]\n",
    "        \n",
    "        pred = model_PersonDetector(img, augment=False, visualize=False)[0]\n",
    "        pred = non_max_suppression(pred, classes=0)\n",
    "        person = 0\n",
    "        for i, det in enumerate(pred):\n",
    "            if len (pred) >1 :\n",
    "                print(\"len(pred) = \",len(pred))\n",
    "            if i > 0:\n",
    "                print(\"first i = \", i)\n",
    "            p, s, im0, frame = path, '', im0s.copy(), getattr(dataset, 'frame', 0)\n",
    "            if len(det):\n",
    "                # Rescale boxes from img_size to im0 size\n",
    "                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "                for *xyxy, conf, cls in reversed(det):\n",
    "                        person += 1\n",
    "                        px1,py1 , px2,py2 = torch.tensor(xyxy).view(1, 4).view(-1).tolist()\n",
    "\n",
    "                        img_crop = im0[int(py1):int(py2+1), int(px1):int(px2+1), :]\n",
    "                        img_in = img_crop[...,::-1] #Convert BGR to RGB(minus1 step size in last dimension)\n",
    "                        img_in = resize_with_pad(img_in, 640, 640)\n",
    "                        img_in = np.moveaxis(img_in, -1, 0)\n",
    "\n",
    "                        img_in = torch.from_numpy(img_in).to('cuda')\n",
    "                        img_in = img_in.float()\n",
    "                        img_in = img_in / 255.0\n",
    "\n",
    "                        if len(img_in.shape) == 3:\n",
    "                            img_in = img_in[None]\n",
    "\n",
    "                        pred_head = model_HeadDetector(img_in, augment=False, visualize=False)[0]\n",
    "                        pred_head = non_max_suppression(pred_head, conf_thres=0.6)\n",
    "\n",
    "                        for _, det in enumerate(pred_head):\n",
    "                            p, s, img0, frame = path, '', img_crop.copy(), getattr(dataset, 'frame', 0)\n",
    "                            if len(det):\n",
    "                                det[:, :4] = scale_coords(img_in.shape[2:], det[:, :4], img0.shape).round()\n",
    "                                for *xyxy, conf, cls in reversed(det):\n",
    "                                        \n",
    "                                    hx1,hy1,hx2,hy2 = torch.tensor(xyxy).view(1, 4).view(-1).tolist()\n",
    "\n",
    "                                    x1 = px1 + hx1\n",
    "                                    y1 = py1 + hy1\n",
    "                                    x2 = px2 - (img_crop.shape[1]-hx2)\n",
    "                                    y2 = py2 - (img_crop.shape[0]-hy2)\n",
    "                                    left_x= x1-((expand_headbox_width-1)*(x2-x1)/2)\n",
    "                                    right_x=x2+((expand_headbox_width-1)*(x2-x1)/2)\n",
    "                                    if left_x < 0:\n",
    "                                        left_x = 0\n",
    "                                    if right_x > px2:\n",
    "                                        right_x =px2\n",
    "                                    # cv2.imwrite(head_save_path +'/'+str(counter)+\".jpg\",im0s[int(y1):int(y2), int(left_x):int(right_x), :])\n",
    "\n",
    "                                    img_crop2 = im0[int(y1):int(y2+1), int(left_x):int(right_x+1), :]\n",
    "                                    img_in2 = img_crop2[...,::-1] #Convert BGR to RGB(minus1 step size in last dimension)\n",
    "                                    img_in2 = resize_with_pad(img_in2, 224, 224)\n",
    "                                    img_in2 = np.moveaxis(img_in2, -1, 0)\n",
    "\n",
    "                                    img_in2 = torch.from_numpy(img_in2).to('cuda')\n",
    "                                    img_in2 = img_in2.float()\n",
    "                                    img_in2 = img_in2 / 255.0\n",
    "                                    \n",
    "                                    if len(img_in2.shape) == 3:\n",
    "                                        img_in2 = img_in2[None]\n",
    "                                                \n",
    "                                    model_muff.eval()\n",
    "                                    scores = model_muff(\n",
    "                                        constant_support_images.cuda(),\n",
    "                                        constant_support_labels.cuda(),\n",
    "                                        img_in2,\n",
    "                                    ).detach()\n",
    "\n",
    "                                    _, predicted_labels = torch.max(scores.data, 1)\n",
    "\n",
    "                                    label = name_classes[predicted_labels[0]]\n",
    "                                    color = color_list[predicted_labels[0]]\n",
    "\n",
    "                                    cv2.rectangle(im0, (int(left_x), int(y1)), (int(right_x), int(y2)), color, 2)\n",
    "                                    # cv2.rectangle(im0, (int(x1), int(y1)-10), (int(x1)+20, int(y1)), (0,0,255), -1)\n",
    "                                    cv2.putText(im0, label, (int(left_x), int(y1)), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 1)\n",
    "                                    \n",
    "                                    if label == 'Muff':\n",
    "                                            cv2.imwrite(Muff_save_path +'/'+str(counter)+\"(\"+str(person)+\")\"+\".jpg\",im0s[int(y1):int(y2), int(left_x):int(right_x), :])\n",
    "                                    else:\n",
    "                                            cv2.imwrite(NotMuff_save_path +'/'+str(counter)+\"(\"+str(person)+\")\"+\".jpg\",im0s[int(y1):int(y2), int(left_x):int(right_x), :])\n",
    "\n",
    "        cv2.imshow('out', im0)\n",
    "        # cv2.imwrite(frame_save_path +\"/\" +str(counter)+\".jpg\",im0)\n",
    "        vid_writer.write(im0)\n",
    "        if cv2.waitKey(1) & 0XFF==ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "vid_writer.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('last_tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "740c9b695977d229d5b62e01484bfaf77d019890a33082bdb8c640459c9cdec2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
