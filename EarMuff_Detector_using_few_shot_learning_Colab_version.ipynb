{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kivV1WFR9Wt0"
      },
      "source": [
        "# Earmuff detector with Few Shot Learning\n",
        "This repo is an earmuff detector. It can get video stream from CCTV in workplaces and detect if the person in the video is using an earmuff or not.\n",
        "First, a trained YoloV5 model detects the person in the video; then, another YoloV5 model detects the person's head. The detected box of a head is sent to a PrototypicalNetwork, which was not trained to detect earmuff previously. The accuracy is acceptable, as you can see in the sample video. However, it should be noted that I have used some pictures of this video as the support set, and this will increase the accuracy but may not be possible in most practical situations.\n",
        "I take the Few Shot Learning(FSL) method because we don't have a dataset. I used efficientnet_b2 network architecture pre-trained on the ImageNet dataset as a feature extractor. In a later work, after a proper dataset is ready for this purpose, another method will be tested in reported here.\n",
        "\n",
        "[Here is the FSL repository](https://github.com/sicara/easy-few-shot-learning) from GitHub that I used. Thanks to @ebennequin, there is a good explanation of few-shot learning in the repository as well. you can [access the google colab file here](https://colab.research.google.com/github/sicara/easy-few-shot-learning/blob/master/notebooks/my_first_few_shot_classifier.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by cloning the repo and installing libraries."
      ],
      "metadata": {
        "id": "gel7i_CYa_b1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/BunNybuger/EarMuff_detector.git\n",
        "%cd EarMuff_detector/"
      ],
      "metadata": {
        "id": "S86sd-Zlb1Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch==1.10.1 torchvision==0.11.2"
      ],
      "metadata": {
        "id": "PABD984Nb4Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "hB15QGc39Wt4"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from models.experimental import attempt_load\n",
        "from utils.datasets import LoadStreams, LoadImages\n",
        "from utils.general import non_max_suppression \n",
        "import torch\n",
        "from utils.general import check_img_size, check_requirements, check_imshow, colorstr, non_max_suppression, \\\n",
        "    apply_classifier, scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
        "import time\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ny4jyOU59Wt5"
      },
      "source": [
        "# Few Shot Learning Model: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "eEXr5dZB9Wt5"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional\n",
        "convert_tensor = transforms.ToTensor()\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import Omniglot\n",
        "from torchvision.models import efficientnet_b2 #resnet18\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "from EasyFSL.easyfsl.data_tools import EasySet, TaskSampler\n",
        "from EasyFSL.easyfsl.utils import plot_images, sliding_average\n",
        "\n",
        "name_classes = ['Muff', 'NotMuff']\n",
        "color_list = [(0,255,0),(0,0,255)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "L76Om0Jf9Wt6"
      },
      "outputs": [],
      "source": [
        "class PrototypicalNetworks(nn.Module):\n",
        "    def __init__(self, backbone: nn.Module):\n",
        "        super(PrototypicalNetworks, self).__init__()\n",
        "        self.backbone = backbone\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        support_images: torch.Tensor,\n",
        "        support_labels: torch.Tensor,\n",
        "        query_images: torch.Tensor,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Predict query labels using labeled support images.\n",
        "        \"\"\"\n",
        "        # Extract the features of support and query images\n",
        "        z_support = self.backbone.forward(support_images)\n",
        "        z_query = self.backbone.forward(query_images)\n",
        "\n",
        "        # Infer the number of different classes from the labels of the support set\n",
        "        n_way = len(torch.unique(support_labels))\n",
        "        # Prototype i is the mean of all instances of features corresponding to labels == i\n",
        "        z_proto = torch.cat(\n",
        "            [\n",
        "                z_support[torch.nonzero(support_labels == label)].mean(0)\n",
        "                for label in range(n_way)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Compute the euclidean distance from queries to prototypes\n",
        "        dists = torch.cdist(z_query, z_proto)\n",
        "\n",
        "        # And here is the super complicated operation to transform those distances into classification scores!\n",
        "        scores = -dists\n",
        "        return scores\n",
        "\n",
        "\n",
        "convolutional_network = efficientnet_b2(pretrained=True)\n",
        "convolutional_network.fc = nn.Flatten()\n",
        "# print(convolutional_network)\n",
        "\n",
        "model_muff = PrototypicalNetworks(convolutional_network).cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsSuVNhZ9Wt7"
      },
      "source": [
        "### Load Constant Support Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "nez_MF1P9Wt7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47ba4e48-a633-4e1d-eb21-79d3f471c12f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "C_N_WAY = 2 # Number of classes in a task\n",
        "C_N_SHOT = 2 # Number of images per class in the support set\n",
        "C_N_QUERY = 0 # Number of images per class in the query set\n",
        "C_N_EVALUATION_TASKS = 1\n",
        "\n",
        "constant_support_set = EasySet(specs_file=\"./SupportData/ConstantSupportDataset.json\", training=False)\n",
        "test_sampler = TaskSampler(\n",
        "    constant_support_set, n_way=C_N_WAY, n_shot=C_N_SHOT, n_query=C_N_QUERY, n_tasks=C_N_EVALUATION_TASKS\n",
        ")\n",
        "\n",
        "constant_support_data_loader = DataLoader(\n",
        "    constant_support_set,\n",
        "    batch_sampler=test_sampler,\n",
        "    num_workers=8,\n",
        "    pin_memory=True,\n",
        "    collate_fn=test_sampler.episodic_collate_fn,\n",
        ")\n",
        "\n",
        "(   constant_support_images,\n",
        "    constant_support_labels,\n",
        "    example1_query_images,\n",
        "    example1_query_labels,\n",
        "    example1_class_ids,\n",
        ") = next(iter(constant_support_data_loader))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Awy7gIXt9Wt8"
      },
      "source": [
        "################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JolwTz3j9Wt8"
      },
      "source": [
        "# Load video YoloV5 models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "h-leq_tQ9Wt9",
        "outputId": "4994d7d9-e32c-4347-ea33-d600f8bb686f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "Model Summary: 396 layers, 35462484 parameters, 0 gradients\n",
            "Fusing layers... \n",
            "Model Summary: 290 layers, 20852934 parameters, 0 gradients\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def resize_with_pad(img, new_w, new_h):\n",
        "\n",
        "    h,w,_ = img.shape\n",
        "\n",
        "    if h > w:\n",
        "        pad_size = h-w\n",
        "        top, bottom = 0,0\n",
        "        left, right = pad_size//2, pad_size//2\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, None, value = 0)\n",
        "\n",
        "    elif w > h:\n",
        "        pad_size = w-h\n",
        "        top, bottom = pad_size//2, pad_size//2\n",
        "        left, right = 0,0\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, None, value = 0)\n",
        "\n",
        "    img = cv2.resize(img, (new_w,new_h))\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "model_PersonDetector = attempt_load('./Person_Detector.pt', map_location='cuda')  # load FP32 model\n",
        "model_HeadDetector = attempt_load('./Head_detector_fromPersonBox_yolov5.pt', map_location='cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "yZSjnQWQ9Wt-"
      },
      "outputs": [],
      "source": [
        "# ======================= Opens the Video file =======================\n",
        "video_folder='./TestVideo/'\n",
        "video_name = 'GunShop_Trim'\n",
        "dataset = LoadImages(video_folder + video_name +'.mp4', img_size=640, stride=64)\n",
        "\n",
        "expand_headbox_width = 1.2 # multiplied to head box width to include earmuff\n",
        "\n",
        "cap = cv2.VideoCapture(video_folder + video_name +\".mp4\")\n",
        "if (cap.isOpened()== False):\n",
        "    print(\"Error opening video file\")\n",
        "video_fps = int(cap.get(5))\n",
        "frame_width = int(cap.get(3))\n",
        "frame_height = int(cap.get(4))\n",
        "\n",
        "# for path, img, im0s, vid_cap, s in dataset:\n",
        "\n",
        "#     video_fps = int(vid_cap.get(5))\n",
        "#     frame_width = int(vid_cap.get(3))\n",
        "#     frame_height = int(vid_cap.get(4))\n",
        "#     break\n",
        "\n",
        "frame_save_path = video_folder+video_name\n",
        "head_save_path = video_folder+video_name +\"/head\"\n",
        "Muff_save_path = video_folder + video_name+\"/Muff\"\n",
        "NotMuff_save_path = video_folder + video_name+\"/NotMuff\"\n",
        "\n",
        "if not os.path.exists(frame_save_path):\n",
        "    os.mkdir(frame_save_path)\n",
        "\n",
        "vid_writer = cv2.VideoWriter(frame_save_path +\"/\" + video_name + '_Out.mp4',cv2.VideoWriter_fourcc(*'mp4v'), video_fps, (frame_width,frame_height))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "nQ8xwU-s9Wt-"
      },
      "outputs": [],
      "source": [
        "# ======================= For each frame does prepration and detects =======================\n",
        "\n",
        "if not os.path.exists(head_save_path):\n",
        "    os.mkdir(head_save_path)\n",
        "if not os.path.exists(Muff_save_path):\n",
        "    os.mkdir(Muff_save_path)\n",
        "if not os.path.exists(NotMuff_save_path):\n",
        "    os.mkdir(NotMuff_save_path)\n",
        "\n",
        "counter = 0\n",
        "for path, img, im0s, vid_cap, s in dataset:\n",
        "    counter += 1\n",
        "    if counter%1==0:\n",
        "        img = torch.from_numpy(img).to('cuda')\n",
        "        im0s_tensortorch = img\n",
        "        img = img.float()  # uint8 to fp16/32\n",
        "        img /= 255.0  \n",
        "        if len(img.shape) == 3:\n",
        "            img = img[None]\n",
        "        \n",
        "        pred = model_PersonDetector(img, augment=False, visualize=False)[0]\n",
        "        pred = non_max_suppression(pred, classes=0)\n",
        "        person = 0\n",
        "        for i, det in enumerate(pred):\n",
        "            if len (pred) >1 :\n",
        "                print(\"len(pred) = \",len(pred))\n",
        "            if i > 0:\n",
        "                print(\"first i = \", i)\n",
        "            p, s, im0, frame = path, '', im0s.copy(), getattr(dataset, 'frame', 0)\n",
        "            if len(det):\n",
        "                # Rescale boxes from img_size to im0 size\n",
        "                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
        "\n",
        "                for *xyxy, conf, cls in reversed(det):\n",
        "                        person += 1\n",
        "                        px1,py1 , px2,py2 = torch.tensor(xyxy).view(1, 4).view(-1).tolist()\n",
        "\n",
        "                        img_crop = im0[int(py1):int(py2+1), int(px1):int(px2+1), :]\n",
        "                        img_in = img_crop[...,::-1] #Convert BGR to RGB(minus1 step size in last dimension)\n",
        "                        img_in = resize_with_pad(img_in, 640, 640)\n",
        "                        img_in = np.moveaxis(img_in, -1, 0)\n",
        "\n",
        "                        img_in = torch.from_numpy(img_in).to('cuda')\n",
        "                        img_in = img_in.float()\n",
        "                        img_in = img_in / 255.0\n",
        "\n",
        "                        if len(img_in.shape) == 3:\n",
        "                            img_in = img_in[None]\n",
        "\n",
        "                        pred_head = model_HeadDetector(img_in, augment=False, visualize=False)[0]\n",
        "                        pred_head = non_max_suppression(pred_head, conf_thres=0.6)\n",
        "\n",
        "                        for _, det in enumerate(pred_head):\n",
        "                            p, s, img0, frame = path, '', img_crop.copy(), getattr(dataset, 'frame', 0)\n",
        "                            if len(det):\n",
        "                                det[:, :4] = scale_coords(img_in.shape[2:], det[:, :4], img0.shape).round()\n",
        "                                for *xyxy, conf, cls in reversed(det):\n",
        "                                        \n",
        "                                    hx1,hy1,hx2,hy2 = torch.tensor(xyxy).view(1, 4).view(-1).tolist()\n",
        "\n",
        "                                    x1 = px1 + hx1\n",
        "                                    y1 = py1 + hy1\n",
        "                                    x2 = px2 - (img_crop.shape[1]-hx2)\n",
        "                                    y2 = py2 - (img_crop.shape[0]-hy2)\n",
        "                                    left_x= x1-((expand_headbox_width-1)*(x2-x1)/2)\n",
        "                                    right_x=x2+((expand_headbox_width-1)*(x2-x1)/2)\n",
        "                                    if left_x < 0:\n",
        "                                        left_x = 0\n",
        "                                    if right_x > px2:\n",
        "                                        right_x =px2\n",
        "                                    # cv2.imwrite(head_save_path +'/'+str(counter)+\".jpg\",im0s[int(y1):int(y2), int(left_x):int(right_x), :])\n",
        "\n",
        "                                    img_crop2 = im0[int(y1):int(y2+1), int(left_x):int(right_x+1), :]\n",
        "                                    img_in2 = img_crop2[...,::-1] #Convert BGR to RGB(minus1 step size in last dimension)\n",
        "                                    img_in2 = resize_with_pad(img_in2, 224, 224)\n",
        "                                    img_in2 = np.moveaxis(img_in2, -1, 0)\n",
        "\n",
        "                                    img_in2 = torch.from_numpy(img_in2).to('cuda')\n",
        "                                    img_in2 = img_in2.float()\n",
        "                                    img_in2 = img_in2 / 255.0\n",
        "                                    \n",
        "                                    if len(img_in2.shape) == 3:\n",
        "                                        img_in2 = img_in2[None]\n",
        "                                                \n",
        "                                    model_muff.eval()\n",
        "                                    scores = model_muff(\n",
        "                                        constant_support_images.cuda(),\n",
        "                                        constant_support_labels.cuda(),\n",
        "                                        img_in2,\n",
        "                                    ).detach()\n",
        "\n",
        "                                    _, predicted_labels = torch.max(scores.data, 1)\n",
        "\n",
        "                                    label = name_classes[predicted_labels[0]]\n",
        "                                    color = color_list[predicted_labels[0]]\n",
        "\n",
        "                                    cv2.rectangle(im0, (int(left_x), int(y1)), (int(right_x), int(y2)), color, 2)\n",
        "                                    # cv2.rectangle(im0, (int(x1), int(y1)-10), (int(x1)+20, int(y1)), (0,0,255), -1)\n",
        "                                    cv2.putText(im0, label, (int(left_x), int(y1)), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 1)\n",
        "                                    \n",
        "                                    if label == 'Muff':\n",
        "                                            cv2.imwrite(Muff_save_path +'/'+str(counter)+\"(\"+str(person)+\")\"+\".jpg\",im0s[int(y1):int(y2), int(left_x):int(right_x), :])\n",
        "                                    else:\n",
        "                                            cv2.imwrite(NotMuff_save_path +'/'+str(counter)+\"(\"+str(person)+\")\"+\".jpg\",im0s[int(y1):int(y2), int(left_x):int(right_x), :])\n",
        "\n",
        "        # cv2.imshow('out', im0)\n",
        "        # cv2.imwrite(frame_save_path +\"/\" +str(counter)+\".jpg\",im0)\n",
        "        vid_writer.write(im0)\n",
        "        if cv2.waitKey(1) & 0XFF==ord('q'):\n",
        "            break\n",
        "\n",
        "cap.release()\n",
        "vid_writer.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.5 ('last_tf')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "740c9b695977d229d5b62e01484bfaf77d019890a33082bdb8c640459c9cdec2"
      }
    },
    "colab": {
      "name": "EarMuff Detector using few shot learning Colab version.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}