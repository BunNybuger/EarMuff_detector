{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kivV1WFR9Wt0"
      },
      "source": [
        "# Earmuff detector with Few-Shot Learning\n",
        "This repo is an earmuff detector. It can get video stream from CCTV in workplaces and detect if the person in the video is using an earmuff or not.\n",
        "First, a trained YoloV5 model detects the person in the video; another YoloV5 model detects the person's head. The detected box of the head is sent to a PrototypicalNetwork, which was not trained to detect earmuff previously. The accuracy is acceptable, as you can see in the sample video. However, it should be noted that I have used some pictures of this video as the support set, and this will increase the accuracy but may not be possible in most practical situations.\n",
        "I take the Few Shot Learning(FSL) method because we don't have a dataset. I used efficientnet_b2 network architecture pre-trained on the ImageNet dataset as a feature extractor. In a later work, after a proper dataset is ready for this purpose, another method will be tested and reported here.\n",
        "\n",
        "[Here is the FSL repository](https://github.com/sicara/easy-few-shot-learning) from GitHub that I used. Thanks to @ebennequin, there is a good explanation of few-shot learning in the repository as well. You can [access the google colab file here](https://colab.research.google.com/github/sicara/easy-few-shot-learning/blob/master/notebooks/my_first_few_shot_classifier.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by installing libraries and cloning git hub repo."
      ],
      "metadata": {
        "id": "gel7i_CYa_b1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch==1.10.1 torchvision==0.11.2"
      ],
      "metadata": {
        "id": "Jms5bKWRhi61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71aa90c0-8738-4ac0-bce9-5008ef2b4803"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.10.1\n",
            "  Downloading torch-1.10.1-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n",
            "\u001b[K     |██████████████████████████████▎ | 834.1 MB 1.2 MB/s eta 0:00:41tcmalloc: large alloc 1147494400 bytes == 0x3a36a000 @  0x7ff3b51de615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |████████████████████████████████| 881.9 MB 15 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.11.2\n",
            "  Downloading torchvision-0.11.2-cp37-cp37m-manylinux1_x86_64.whl (23.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.3 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.1) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.11.2) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.11.2) (1.21.6)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.0+cu113\n",
            "    Uninstalling torch-1.12.0+cu113:\n",
            "      Successfully uninstalled torch-1.12.0+cu113\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.13.0+cu113\n",
            "    Uninstalling torchvision-0.13.0+cu113:\n",
            "      Successfully uninstalled torchvision-0.13.0+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.10.1 which is incompatible.\n",
            "torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.10.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.10.1 torchvision-0.11.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/BunNybuger/EarMuff_detector.git\n",
        "%cd EarMuff_detector/"
      ],
      "metadata": {
        "id": "S86sd-Zlb1Ik",
        "outputId": "46626d45-e3f4-4fb7-bc7a-06212f7f41a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'EarMuff_detector'...\n",
            "remote: Enumerating objects: 310, done.\u001b[K\n",
            "remote: Counting objects: 100% (165/165), done.\u001b[K\n",
            "remote: Compressing objects: 100% (146/146), done.\u001b[K\n",
            "remote: Total 310 (delta 69), reused 88 (delta 16), pack-reused 145\u001b[K\n",
            "Receiving objects: 100% (310/310), 114.65 MiB | 17.49 MiB/s, done.\n",
            "Resolving deltas: 100% (103/103), done.\n",
            "/content/EarMuff_detector\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hB15QGc39Wt4",
        "outputId": "1306bada-4057-4056-fa1b-1dfe8bcdfe25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from models.experimental import attempt_load\n",
        "from utils.datasets import LoadStreams, LoadImages\n",
        "from utils.general import non_max_suppression \n",
        "import torch\n",
        "from utils.general import check_img_size, check_requirements, check_imshow, colorstr, non_max_suppression, \\\n",
        "    apply_classifier, scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
        "import time\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ny4jyOU59Wt5"
      },
      "source": [
        "# Few Shot Learning Model: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eEXr5dZB9Wt5"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional\n",
        "convert_tensor = transforms.ToTensor()\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import Omniglot\n",
        "from torchvision.models import efficientnet_b2 #resnet18\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "from EasyFSL.easyfsl.data_tools import EasySet, TaskSampler\n",
        "from EasyFSL.easyfsl.utils import plot_images, sliding_average\n",
        "\n",
        "name_classes = ['Muff', 'NotMuff']\n",
        "color_list = [(0,255,0),(0,0,255)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L76Om0Jf9Wt6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "5f0eec80f3b34f40b3efffdf57530482",
            "71d92fcec5be4f239e645123bc9ffa3a",
            "2caded518c914876baa4a1bfe9157c29",
            "d0e6d72f91cc49e595cd0c1f162fd688",
            "7e907cba9b2d45699a2f615a00d3555d",
            "68235d116cc046e09abc1ae84d8a6e24",
            "a07e7aee1a894af2bf1bae1d0f32977f",
            "aa5ffa19b4b84a8abe279d1962f0b8c2",
            "6a8c2a27f8e64d35aa0cc20fab9a42a7",
            "9605c4224c2e4bf1ace9f1d4499ec0ab",
            "2b1d0cc8fb204a6eb0f89b87454ed347"
          ]
        },
        "outputId": "7b17194d-fb95-4c20-eb7e-8b8a8831170c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b2_rwightman-bcdf34b7.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b2_rwightman-bcdf34b7.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/35.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f0eec80f3b34f40b3efffdf57530482"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "class PrototypicalNetworks(nn.Module):\n",
        "    def __init__(self, backbone: nn.Module):\n",
        "        super(PrototypicalNetworks, self).__init__()\n",
        "        self.backbone = backbone\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        support_images: torch.Tensor,\n",
        "        support_labels: torch.Tensor,\n",
        "        query_images: torch.Tensor,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Predict query labels using labeled support images.\n",
        "        \"\"\"\n",
        "        # Extract the features of support and query images\n",
        "        z_support = self.backbone.forward(support_images)\n",
        "        z_query = self.backbone.forward(query_images)\n",
        "\n",
        "        # Infer the number of different classes from the labels of the support set\n",
        "        n_way = len(torch.unique(support_labels))\n",
        "        # Prototype i is the mean of all instances of features corresponding to labels == i\n",
        "        z_proto = torch.cat(\n",
        "            [\n",
        "                z_support[torch.nonzero(support_labels == label)].mean(0)\n",
        "                for label in range(n_way)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Compute the euclidean distance from queries to prototypes\n",
        "        dists = torch.cdist(z_query, z_proto)\n",
        "\n",
        "        # And here is the super complicated operation to transform those distances into classification scores!\n",
        "        scores = -dists\n",
        "        return scores\n",
        "\n",
        "\n",
        "convolutional_network = efficientnet_b2(pretrained=True)\n",
        "convolutional_network.fc = nn.Flatten()\n",
        "# print(convolutional_network)\n",
        "\n",
        "model_muff = PrototypicalNetworks(convolutional_network).cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsSuVNhZ9Wt7"
      },
      "source": [
        "### Load Constant Support Dataset:\n",
        "<a id='Load_Support_Dataset'></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nez_MF1P9Wt7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "725d851d-e6bf-4f99-85f2-05bd8a618b79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "C_N_WAY = 2 # Number of classes in a task\n",
        "C_N_SHOT = 2 # Number of images per class in the support set\n",
        "C_N_QUERY = 0 # Number of images per class in the query set\n",
        "C_N_EVALUATION_TASKS = 1\n",
        "\n",
        "constant_support_set = EasySet(specs_file=\"./SupportData/ConstantSupportDataset.json\", training=False)\n",
        "test_sampler = TaskSampler(\n",
        "    constant_support_set, n_way=C_N_WAY, n_shot=C_N_SHOT, n_query=C_N_QUERY, n_tasks=C_N_EVALUATION_TASKS\n",
        ")\n",
        "\n",
        "constant_support_data_loader = DataLoader(\n",
        "    constant_support_set,\n",
        "    batch_sampler=test_sampler,\n",
        "    num_workers=8,\n",
        "    pin_memory=True,\n",
        "    collate_fn=test_sampler.episodic_collate_fn,\n",
        ")\n",
        "\n",
        "(   constant_support_images,\n",
        "    constant_support_labels,\n",
        "    example1_query_images,\n",
        "    example1_query_labels,\n",
        "    example1_class_ids,\n",
        ") = next(iter(constant_support_data_loader))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Awy7gIXt9Wt8"
      },
      "source": [
        "################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JolwTz3j9Wt8"
      },
      "source": [
        "# Load video YoloV5 models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "h-leq_tQ9Wt9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a84ec53f-c109-4a92-e72c-c403ec12c56f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "Model Summary: 396 layers, 35462484 parameters, 0 gradients\n",
            "Fusing layers... \n",
            "Model Summary: 290 layers, 20852934 parameters, 0 gradients\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def resize_with_pad(img, new_w, new_h):\n",
        "\n",
        "    h,w,_ = img.shape\n",
        "\n",
        "    if h > w:\n",
        "        pad_size = h-w\n",
        "        top, bottom = 0,0\n",
        "        left, right = pad_size//2, pad_size//2\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, None, value = 0)\n",
        "\n",
        "    elif w > h:\n",
        "        pad_size = w-h\n",
        "        top, bottom = pad_size//2, pad_size//2\n",
        "        left, right = 0,0\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, None, value = 0)\n",
        "\n",
        "    img = cv2.resize(img, (new_w,new_h))\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "model_PersonDetector = attempt_load('./Person_Detector.pt', map_location='cuda')  # load FP32 model\n",
        "model_HeadDetector = attempt_load('./Head_detector_fromPersonBox_yolov5.pt', map_location='cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# You can upload and try code on your own video here"
      ],
      "metadata": {
        "id": "3KJ73NRDlw2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#upload your video in /content/EarMuff_detector/TestVideo/\n",
        "# put the name of video here\n",
        "video_name = 'Muff_iStock'\n",
        "video_format = '.mp4'\n"
      ],
      "metadata": {
        "id": "Y6By8wQwlwN1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yZSjnQWQ9Wt-"
      },
      "outputs": [],
      "source": [
        "# ======================= Opens the Video file =======================\n",
        "video_folder='./TestVideo/'\n",
        "dataset = LoadImages(video_folder + video_name +video_format, img_size=640, stride=64)\n",
        "\n",
        "expand_headbox_width = 1.2 # multiplied to head box width to include earmuff\n",
        "\n",
        "cap = cv2.VideoCapture(video_folder + video_name + video_format)\n",
        "if (cap.isOpened()== False):\n",
        "    print(\"Error opening video file\")\n",
        "video_fps = int(cap.get(5))\n",
        "frame_width = int(cap.get(3))\n",
        "frame_height = int(cap.get(4))\n",
        "\n",
        "# for path, img, im0s, vid_cap, s in dataset:\n",
        "\n",
        "#     video_fps = int(vid_cap.get(5))\n",
        "#     frame_width = int(vid_cap.get(3))\n",
        "#     frame_height = int(vid_cap.get(4))\n",
        "#     break\n",
        "\n",
        "frame_save_path = video_folder+video_name\n",
        "# head_save_path = video_folder+video_name +\"/head\"\n",
        "Muff_save_path = video_folder + video_name+\"/Muff\"\n",
        "NotMuff_save_path = video_folder + video_name+\"/NotMuff\"\n",
        "\n",
        "if not os.path.exists(frame_save_path):\n",
        "    os.mkdir(frame_save_path)\n",
        "\n",
        "vid_writer = cv2.VideoWriter(frame_save_path +\"/\" + video_name + '_Out.mp4',cv2.VideoWriter_fourcc(*'mp4v'), video_fps, (frame_width,frame_height))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nQ8xwU-s9Wt-"
      },
      "outputs": [],
      "source": [
        "# ======================= For each frame does prepration and detects =======================\n",
        "\n",
        "# if not os.path.exists(head_save_path):\n",
        "#     os.mkdir(head_save_path)\n",
        "if not os.path.exists(Muff_save_path):\n",
        "    os.mkdir(Muff_save_path)\n",
        "if not os.path.exists(NotMuff_save_path):\n",
        "    os.mkdir(NotMuff_save_path)\n",
        "\n",
        "counter = 0\n",
        "for path, img, im0s, vid_cap, s in dataset:\n",
        "    counter += 1\n",
        "    if counter%1==0:\n",
        "        img = torch.from_numpy(img).to('cuda')\n",
        "        im0s_tensortorch = img\n",
        "        img = img.float()  # uint8 to fp16/32\n",
        "        img /= 255.0  \n",
        "        if len(img.shape) == 3:\n",
        "            img = img[None]\n",
        "        \n",
        "        pred = model_PersonDetector(img, augment=False, visualize=False)[0]\n",
        "        pred = non_max_suppression(pred, classes=0)\n",
        "        person = 0\n",
        "        for i, det in enumerate(pred):\n",
        "            if len (pred) >1 :\n",
        "                print(\"len(pred) = \",len(pred))\n",
        "            if i > 0:\n",
        "                print(\"first i = \", i)\n",
        "            p, s, im0, frame = path, '', im0s.copy(), getattr(dataset, 'frame', 0)\n",
        "            if len(det):\n",
        "                # Rescale boxes from img_size to im0 size\n",
        "                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
        "\n",
        "                for *xyxy, conf, cls in reversed(det):\n",
        "                        person += 1\n",
        "                        px1,py1 , px2,py2 = torch.tensor(xyxy).view(1, 4).view(-1).tolist()\n",
        "\n",
        "                        img_crop = im0[int(py1):int(py2+1), int(px1):int(px2+1), :]\n",
        "                        img_in = img_crop[...,::-1] #Convert BGR to RGB(minus1 step size in last dimension)\n",
        "                        img_in = resize_with_pad(img_in, 640, 640)\n",
        "                        img_in = np.moveaxis(img_in, -1, 0)\n",
        "\n",
        "                        img_in = torch.from_numpy(img_in).to('cuda')\n",
        "                        img_in = img_in.float()\n",
        "                        img_in = img_in / 255.0\n",
        "\n",
        "                        if len(img_in.shape) == 3:\n",
        "                            img_in = img_in[None]\n",
        "\n",
        "                        pred_head = model_HeadDetector(img_in, augment=False, visualize=False)[0]\n",
        "                        pred_head = non_max_suppression(pred_head, conf_thres=0.6)\n",
        "\n",
        "                        for _, det in enumerate(pred_head):\n",
        "                            p, s, img0, frame = path, '', img_crop.copy(), getattr(dataset, 'frame', 0)\n",
        "                            if len(det):\n",
        "                                det[:, :4] = scale_coords(img_in.shape[2:], det[:, :4], img0.shape).round()\n",
        "                                for *xyxy, conf, cls in reversed(det):\n",
        "                                        \n",
        "                                    hx1,hy1,hx2,hy2 = torch.tensor(xyxy).view(1, 4).view(-1).tolist()\n",
        "\n",
        "                                    x1 = px1 + hx1\n",
        "                                    y1 = py1 + hy1\n",
        "                                    x2 = px2 - (img_crop.shape[1]-hx2)\n",
        "                                    y2 = py2 - (img_crop.shape[0]-hy2)\n",
        "                                    left_x= x1-((expand_headbox_width-1)*(x2-x1)/2)\n",
        "                                    right_x=x2+((expand_headbox_width-1)*(x2-x1)/2)\n",
        "                                    if left_x < 0:\n",
        "                                        left_x = 0\n",
        "                                    if right_x > px2:\n",
        "                                        right_x =px2\n",
        "                                    # cv2.imwrite(head_save_path +'/'+str(counter)+\".jpg\",im0s[int(y1):int(y2), int(left_x):int(right_x), :])\n",
        "\n",
        "                                    img_crop2 = im0[int(y1):int(y2+1), int(left_x):int(right_x+1), :]\n",
        "                                    img_in2 = img_crop2[...,::-1] #Convert BGR to RGB(minus1 step size in last dimension)\n",
        "                                    img_in2 = resize_with_pad(img_in2, 224, 224)\n",
        "                                    img_in2 = np.moveaxis(img_in2, -1, 0)\n",
        "\n",
        "                                    img_in2 = torch.from_numpy(img_in2).to('cuda')\n",
        "                                    img_in2 = img_in2.float()\n",
        "                                    img_in2 = img_in2 / 255.0\n",
        "                                    \n",
        "                                    if len(img_in2.shape) == 3:\n",
        "                                        img_in2 = img_in2[None]\n",
        "                                                \n",
        "                                    model_muff.eval()\n",
        "                                    scores = model_muff(\n",
        "                                        constant_support_images.cuda(),\n",
        "                                        constant_support_labels.cuda(),\n",
        "                                        img_in2,\n",
        "                                    ).detach()\n",
        "\n",
        "                                    _, predicted_labels = torch.max(scores.data, 1)\n",
        "\n",
        "                                    label = name_classes[predicted_labels[0]]\n",
        "                                    color = color_list[predicted_labels[0]]\n",
        "\n",
        "                                    cv2.rectangle(im0, (int(left_x), int(y1)), (int(right_x), int(y2)), color, 2)\n",
        "                                    # cv2.rectangle(im0, (int(x1), int(y1)-10), (int(x1)+20, int(y1)), (0,0,255), -1)\n",
        "                                    cv2.putText(im0, label, (int(left_x), int(y1)), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 1)\n",
        "                                    \n",
        "                                    if label == 'Muff':\n",
        "                                            cv2.imwrite(Muff_save_path +'/'+str(counter)+\"(\"+str(person)+\")\"+\".jpg\",im0s[int(y1):int(y2), int(left_x):int(right_x), :])\n",
        "                                    else:\n",
        "                                            cv2.imwrite(NotMuff_save_path +'/'+str(counter)+\"(\"+str(person)+\")\"+\".jpg\",im0s[int(y1):int(y2), int(left_x):int(right_x), :])\n",
        "\n",
        "        # cv2.imshow('out', im0)\n",
        "        # cv2.imwrite(frame_save_path +\"/\" +str(counter)+\".jpg\",im0)\n",
        "        vid_writer.write(im0)\n",
        "        if cv2.waitKey(1) & 0XFF==ord('q'):\n",
        "            break\n",
        "\n",
        "cap.release()\n",
        "vid_writer.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output video and head boxes labeled as Muff and NotMuff are now saved in a TestVideo, in a folder with the same name as your input video.  \n",
        "Note that few-shot learning is very dependent on SupportData. So you should create a tiny SupportData set from your video to get better results. To create a SupportData set, follow the below steps.  \n",
        "\n",
        "1- Open TestVideo\\\"your video_name\"  \n",
        "2- Inside, there is a Muff and a NotMuff folder - choose some pics that are detected wrongly and download them.  \n",
        "3- Go to the SupportData folder and delete all pics in Muff and NotMuff folder.  \n",
        "4- Upload some of those photos you selected from your own video in Muff and NotMuff.Make sure labels are correct.  \n",
        "5- re-run this file from the [Load Constant Support Dataset](#Load_Support_Dataset) cell down here.  "
      ],
      "metadata": {
        "id": "F5Yt37yWgFAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DLxu8cUQi7T9"
      },
      "execution_count": 9,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.5 ('last_tf')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "740c9b695977d229d5b62e01484bfaf77d019890a33082bdb8c640459c9cdec2"
      }
    },
    "colab": {
      "name": "EarMuff Detector using few shot learning Colab version.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5f0eec80f3b34f40b3efffdf57530482": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_71d92fcec5be4f239e645123bc9ffa3a",
              "IPY_MODEL_2caded518c914876baa4a1bfe9157c29",
              "IPY_MODEL_d0e6d72f91cc49e595cd0c1f162fd688"
            ],
            "layout": "IPY_MODEL_7e907cba9b2d45699a2f615a00d3555d"
          }
        },
        "71d92fcec5be4f239e645123bc9ffa3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68235d116cc046e09abc1ae84d8a6e24",
            "placeholder": "​",
            "style": "IPY_MODEL_a07e7aee1a894af2bf1bae1d0f32977f",
            "value": "100%"
          }
        },
        "2caded518c914876baa4a1bfe9157c29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa5ffa19b4b84a8abe279d1962f0b8c2",
            "max": 36882185,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6a8c2a27f8e64d35aa0cc20fab9a42a7",
            "value": 36882185
          }
        },
        "d0e6d72f91cc49e595cd0c1f162fd688": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9605c4224c2e4bf1ace9f1d4499ec0ab",
            "placeholder": "​",
            "style": "IPY_MODEL_2b1d0cc8fb204a6eb0f89b87454ed347",
            "value": " 35.2M/35.2M [00:00&lt;00:00, 182MB/s]"
          }
        },
        "7e907cba9b2d45699a2f615a00d3555d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68235d116cc046e09abc1ae84d8a6e24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a07e7aee1a894af2bf1bae1d0f32977f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa5ffa19b4b84a8abe279d1962f0b8c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a8c2a27f8e64d35aa0cc20fab9a42a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9605c4224c2e4bf1ace9f1d4499ec0ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b1d0cc8fb204a6eb0f89b87454ed347": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}